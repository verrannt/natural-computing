{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define below two functions for loading the data and scoring the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename:str):\n",
    "    \"\"\" \n",
    "    Helper function that loads data from storage given \n",
    "    a filename that contains a full path to the file. \n",
    "    Assumes that each datapoints is stored as individual line\n",
    "    in the file.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data_list.append(line[:-1])\n",
    "    return data_list\n",
    "   \n",
    "# Load the testdata for English and Tagalog\n",
    "english_test = load_data('english.test')\n",
    "tagalog_test = load_data('tagalog.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(train_name, test_name, seq_length=10, r=4):\n",
    "    \"\"\" \n",
    "    Run the Negative Selection algorithm implemented in Java.\n",
    "    This issues a system call to a subprocess with the arguments needed\n",
    "    to run the Java program. \n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "    train_name: The file name (full path) to the training set\n",
    "    test_name: The file name (full path) to the test set\n",
    "    seq_length: The length of the sequences in the sets\n",
    "    r: Parameter r of the Negative Selection algorithm\n",
    "    \n",
    "    RETURNS\n",
    "    =======\n",
    "    The score for each of the datapoints in the testset\n",
    "    \"\"\"\n",
    "    # Define the command to run the algorithm with Java\n",
    "    run_command = f\"java -jar negsel2.jar -self {train_name} -n {seq_length} -r {r} -c -l < {test_name}\"\n",
    "    # Issue call to subprocess to run the command\n",
    "    results = subprocess.getoutput(run_command)\n",
    "    # Convert the results to numpy array of floats\n",
    "    return np.array([float(r) for r in results.split('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 1 & 2\n",
    "Below is the code to answer exercises 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score for r=1: 0.5435\n",
      "ROC AUC Score for r=2: 0.7396\n",
      "ROC AUC Score for r=3: 0.8311\n",
      "ROC AUC Score for r=4: 0.7916\n",
      "ROC AUC Score for r=5: 0.7282\n",
      "ROC AUC Score for r=6: 0.6681\n",
      "ROC AUC Score for r=7: 0.5907\n",
      "ROC AUC Score for r=8: 0.5202\n",
      "ROC AUC Score for r=9: 0.5121\n",
      "\n",
      "The best score is observed for r=3 with 0.8311\n"
     ]
    }
   ],
   "source": [
    "# Collect the score for the best r value\n",
    "best_score = 0\n",
    "best_r = None\n",
    "\n",
    "# Iterate through all r values in range 1 to 10\n",
    "for r in range(1,10):\n",
    "    \n",
    "    # Train the algorithm on english data, and test on tagalog and english\n",
    "    testresults_tagalog = get_scores('english.train', 'tagalog.test', r=r)\n",
    "    testresults_english = get_scores('english.train', 'english.test', r=r)\n",
    "\n",
    "    # Concatenate the results from both languages\n",
    "    testresults = np.concatenate((testresults_tagalog, testresults_english))\n",
    "    \n",
    "    # Create boolean labels for both datasets (1 for tagalog, 0 for english)\n",
    "    labels = np.zeros(testresults.shape[0], dtype=bool)\n",
    "    labels[:testresults_tagalog.shape[0]] = True\n",
    "    \n",
    "    # Compute the ROC AUC score for this setting\n",
    "    ras = roc_auc_score(labels, testresults)\n",
    "    \n",
    "    if ras > best_score:\n",
    "        best_r = r\n",
    "        best_score = ras\n",
    "    \n",
    "    print(f'ROC AUC Score for r={r}: {ras:.4f}')\n",
    "    \n",
    "print(f'\\nThe best score is observed for r={best_r} with {best_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for r=1 the score is pretty low, which can be explained by the fact that it matches too many strings and is thus underfitting.\n",
    "\n",
    "For r=9 we observe an equally bad score, which makes sense considering we have 10 letter strings and are thus overfitting on the provided training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.\n",
    "\n",
    "**Q:** The folder `lang` contains strings from 4 other languages. Determine which of these languages can be best\n",
    "discriminated from English using the negative selection algorithm, and for which of the languages this is\n",
    "most difficult. Can you explain your findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory to the languages\n",
    "languages_dir = 'lang/'\n",
    "# The names of the individual languages\n",
    "languages = ['hiligaynon.txt', 'middle-english.txt', 'plautdietsch.txt', 'xhosa.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer the question, we select the subrange 2 to 6 of the previous tested r values since values outside of this range did not gather good performance. We loop through all of these values and collect the english test results. Inside of each loop, we loop again through all of the four provided alternative languages. The code inside this second for-loop is the same as above: compute the test score for the alternative language, create labels, and compute the ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for r=2:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.752\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.514\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.707\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.852\n",
      "Computing for r=3:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.840\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.542\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.775\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.889\n",
      "Computing for r=4:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.797\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.534\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.753\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.832\n",
      "Computing for r=5:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.730\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.522\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.701\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.765\n",
      "Computing for r=6:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.671\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.502\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.650\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.692\n"
     ]
    }
   ],
   "source": [
    "for r in [2,3,4,5,6]:\n",
    "    print(f'Computing for r={r}:')\n",
    "\n",
    "    testresults_english = get_scores('english.train', 'english.test', r=r)\n",
    "\n",
    "    for language in languages:\n",
    "        testresults_lang = get_scores('english.train', languages_dir+language, r=r)\n",
    "\n",
    "        testresults = np.concatenate((testresults_lang, testresults_english))\n",
    "\n",
    "        labels = np.zeros(testresults.shape[0], dtype=bool)\n",
    "        labels[:testresults_lang.shape[0]] = True\n",
    "\n",
    "        ras = roc_auc_score(labels, testresults)\n",
    "\n",
    "        print(f'\\tROC AUC Score for language \"{language}\": {ras:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **For the full written answer, please consult our PDF hand-in.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
