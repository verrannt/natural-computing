{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename:str):\n",
    "    \"\"\" \n",
    "    Helper function that loads data from storage given \n",
    "    a filename that contains a full path to the file. \n",
    "    Assumes that each datapoints is stored as individual line\n",
    "    in the file.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data_list.append(line[:-1])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path variables below that denote which data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory that contains everything related to this exercise\n",
    "syscalls_dir = 'syscalls/'\n",
    "# Specify the name of the dataset ('snd-cert' or 'snd-unm')\n",
    "dataset_name = 'snd-unm'\n",
    "# Specify which test dataset to use (1, 2, or 3 for each of the\n",
    "# above datsets)\n",
    "testdata_number = 3\n",
    "# All data files have the same sub-path so we can reuse it\n",
    "path_to_data_files = syscalls_dir+dataset_name+'/'+dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_name = path_to_data_files+'.train'\n",
    "train_data = load_data(train_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_name = path_to_data_files+f'.{testdata_number}.test'\n",
    "test_data = load_data(test_name)\n",
    "test_data_len = len(test_data)\n",
    "\n",
    "# Load test labels\n",
    "test_labels = path_to_data_files+f'.{testdata_number}.labels'\n",
    "test_labels = load_data(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the minimum sequence length of both train and test data. This is needed for computing the chunks later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sequence in both train and test set: 7\n"
     ]
    }
   ],
   "source": [
    "min_seq_len = min(min([len(s) for s in train_data]), min([len(s) for s in test_data]))\n",
    "print('Shortest sequence in both train and test set:', min_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sequence(sequence:str, chunk_size:int):\n",
    "    \"\"\" \n",
    "    Chunk a sequence into subsequences of length <chunk_size>.\n",
    "    If the last subsequence is shorter than <chunk_size> it is\n",
    "    dropped. \n",
    "    Note: we assume that the sequence is at least of length\n",
    "    <chunk_size> because we compute the minimum sequence length\n",
    "    above. If it is shorter, this function returns an empty list.\n",
    "    \"\"\"\n",
    "    # Special case for short sequences\n",
    "    if len(sequence)==chunk_size:\n",
    "        return [sequence]\n",
    "    \n",
    "    # Compute chunks from sequence\n",
    "    chunks = []\n",
    "    for i in range(chunk_size, len(sequence), chunk_size):\n",
    "        chunks.append(sequence[i-chunk_size:i])\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we chunk each sequence in the training data into chunks of specified length `min_seq_len`. We use list comprehension to do this is a fast way, an simultanously flatten the list. Thus, `train_data` will be a non-nested list of sequences of length `min_seq_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(set([seqq for seq in train_data for seqq in chunk_sequence(seq, min_seq_len)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same below for the test data. Note however, that we cannot simply chunk the data and flatten the list like above, because we need to keep track of the original sequence a chunk belongs to in order to later on compute the average score over all chunks of a sequence.\n",
    "\n",
    "To achieve this, we first chunk all sequences in `test_data` without flattening the list.\n",
    "\n",
    "Then, we flatten the list, but record the index of the original sequence together with the actual chunk string as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all sequences without flattening\n",
    "test_data = [chunk_sequence(seq, min_seq_len) for seq in test_data]\n",
    "# Flatten the list but record chunk together with index of sequence\n",
    "# it belongs to\n",
    "test_data = [(chunk,idx) for idx,sublist in enumerate(test_data) for chunk in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Disk\n",
    "We need to save the datasets to disk such that the call to Java can pick them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write chunked train data to file\n",
    "with open(train_name+'.chunked', 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write chunked test data to file\n",
    "with open(test_name+'.chunked', 'w') as f:\n",
    "    for chunk, idx in test_data:\n",
    "        f.write(chunk)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the function to run the algorithm given a train and test set name, and the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(train_name, test_name, seq_length, r=4):\n",
    "    \"\"\" \n",
    "    Run the Negative Selection algorithm implemented in Java.\n",
    "    This issues a system call to a subprocess with the arguments needed\n",
    "    to run the Java program. \n",
    "    \n",
    "    PARAMS\n",
    "    ======\n",
    "    train_name: The file name (full path) to the training set\n",
    "    test_name: The file name (full path) to the test set\n",
    "    seq_length: The length of the sequences in the sets\n",
    "    r: Parameter r of the Negative Selection algorithm\n",
    "    \n",
    "    RETURNS\n",
    "    =======\n",
    "    The score for each of the datapoints in the testset\n",
    "    \"\"\"\n",
    "    # Define the command to run the algorithm with Java\n",
    "    run_command = f\"java -jar negsel2.jar -self {train_name} -n {seq_length} -r {r} -c -l < {test_name}\"\n",
    "    # Issue call to subprocess to run the command\n",
    "    results = subprocess.getoutput(run_command)\n",
    "    # Convert the results to numpy array of floats\n",
    "    return np.array([float(r) for r in results.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_scores(train_name+'.chunked', test_name+'.chunked', seq_length=min_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the scores are a one-dimensional list, containing a score for each chunk. We now want to map these chunk-scores back to the sequence they belong to, and then compute the average score per sequence.\n",
    "\n",
    "To do this, we first create a list `unnested_scores` that contains an empty list for every element in the original test data. Then, we populate this list by appending the score from `scores` at the correct index. We obtain the correct index from the `test_data` that we populated with tuples in cell 9. Lastly, we compute the average for each sequence simply by taking the mean of each of the lists in `unnested_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average scores for sequences from chunks\n",
    "unnested_scores = [list() for _ in range(test_data_len)]\n",
    "\n",
    "for i,score in enumerate(scores):\n",
    "    unnested_scores[test_data[i][1]].append(score)\n",
    "    \n",
    "avg_scores = [np.mean(sublist) for sublist in unnested_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the ROC score for this fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98529"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute ROC score\n",
    "roc_auc_score(test_labels, avg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
