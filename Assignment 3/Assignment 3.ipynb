{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = []\n",
    "\n",
    "def load_data(filename):\n",
    "    data_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data_list.append(line[:-1])\n",
    "    return data_list\n",
    "    \n",
    "english_test = load_data('english.test')\n",
    "tagalog_test = load_data('tagalog.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openjdk version \"1.8.0_282\"\\nOpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~20.04-b08)\\nOpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.getoutput(\"java -version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average of the tagalog testset\n",
    "avg = !java -jar negsel2.jar -self english.train -n 10 -r 4 -c -l < tagalog.test | awk '{n+=$1}END{print n/NR}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get individual test scores\n",
    "\n",
    "\n",
    "testresults_tagalog = !java -jar negsel2.jar -self english.train -n 10 -r 4 -c -l < tagalog.test\n",
    "testresults_tagalog = np.array([float(t) for t in testresults_tagalog])\n",
    "\n",
    "testresults_english = !java -jar negsel2.jar -self english.train -n 10 -r 4 -c -l < english.test\n",
    "testresults_english = np.array([float(t) for t in testresults_english])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 31.28750387473102\n"
     ]
    }
   ],
   "source": [
    "global_min = min([min(testresults_english), min(testresults_tagalog)])\n",
    "global_max = max([max(testresults_english), max(testresults_tagalog)])\n",
    "print(global_min, global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize (need not scale by min because it's 0)\n",
    "testresults_tagalog /= global_max\n",
    "testresults_english /= global_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testresults = np.concatenate((testresults_tagalog, testresults_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(testresults.shape[0], dtype=bool)\n",
    "labels[:testresults_tagalog.shape[0]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7916097138691454"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(labels, testresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(filename, r=4):\n",
    "    run_command = f\"java -jar negsel2.jar -self english.train -n 10 -r {r} -c -l < {filename}\"\n",
    "    results = subprocess.getoutput(run_command)\n",
    "    return np.array([float(r) for r in results.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score for r=1: 0.5435347184253692\n",
      "ROC AUC Score for r=2: 0.7396459814106069\n",
      "ROC AUC Score for r=3: 0.8311235647895024\n",
      "ROC AUC Score for r=4: 0.7916097138691454\n",
      "ROC AUC Score for r=5: 0.7282440313468198\n",
      "ROC AUC Score for r=6: 0.6680847913249499\n",
      "ROC AUC Score for r=7: 0.5907258064516129\n",
      "ROC AUC Score for r=8: 0.5201612903225806\n",
      "ROC AUC Score for r=9: 0.5120967741935484\n"
     ]
    }
   ],
   "source": [
    "for r in range(1,10):\n",
    "    testresults_tagalog = get_scores('tagalog.test', r)\n",
    "    testresults_english = get_scores('english.test', r)\n",
    "    \n",
    "    #global_min = min([min(testresults_english), min(testresults_tagalog)])\n",
    "    #global_max = max([max(testresults_english), max(testresults_tagalog)])\n",
    "    \n",
    "    # Normalize (need not scale by min because it's 0)\n",
    "    #testresults_tagalog /= global_max\n",
    "    #testresults_english /= global_max\n",
    "    \n",
    "    testresults = np.concatenate((testresults_tagalog, testresults_english))\n",
    "    \n",
    "    labels = np.zeros(testresults.shape[0], dtype=bool)\n",
    "    labels[:testresults_tagalog.shape[0]] = True\n",
    "    \n",
    "    ras = roc_auc_score(labels, testresults)\n",
    "    \n",
    "    print(f'ROC AUC Score for r={r}:', ras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The best score is observed for r=3\n",
    "* We observe that for r=1 the score is pretty low, which can be explained by the fact that it matches too many strings and is thus underfitting\n",
    "* For r=9 we observe an equally bad score, which makes sense considering we have 10 letter strings and are thus overfitting on the provided training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.\n",
    "\n",
    "The folder `lang` contains strings from 4 other languages. Determine which of these languages can be best\n",
    "discriminated from English using the negative selection algorithm, and for which of the languages this is\n",
    "most difficult. Can you explain your findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_dir = 'lang/'\n",
    "languages = ['hiligaynon.txt', 'middle-english.txt', 'plautdietsch.txt', 'xhosa.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for r=2:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.752\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.514\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.707\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.852\n",
      "Computing for r=3:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.840\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.542\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.775\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.889\n",
      "Computing for r=4:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.797\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.534\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.753\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.832\n",
      "Computing for r=5:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.730\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.522\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.701\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.765\n",
      "Computing for r=6:\n",
      "\tROC AUC Score for language \"hiligaynon.txt\": 0.671\n",
      "\tROC AUC Score for language \"middle-english.txt\": 0.502\n",
      "\tROC AUC Score for language \"plautdietsch.txt\": 0.650\n",
      "\tROC AUC Score for language \"xhosa.txt\": 0.692\n"
     ]
    }
   ],
   "source": [
    "for r in [2,3,4,5,6]:\n",
    "    print(f'Computing for r={r}:')\n",
    "\n",
    "    testresults_english = get_scores('english.test', r)\n",
    "\n",
    "    for language in languages:\n",
    "        testresults_lang = get_scores(languages_dir+language, r)\n",
    "\n",
    "        testresults = np.concatenate((testresults_lang, testresults_english))\n",
    "\n",
    "        labels = np.zeros(testresults.shape[0], dtype=bool)\n",
    "        labels[:testresults_lang.shape[0]] = True\n",
    "\n",
    "        ras = roc_auc_score(labels, testresults)\n",
    "\n",
    "        print(f'\\tROC AUC Score for language \"{language}\": {ras:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
